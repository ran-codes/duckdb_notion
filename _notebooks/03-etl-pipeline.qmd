---
title: "ETL Pipeline: CSV to Notion"
format: html
jupyter: python3
---

# Building an ETL Pipeline with DuckDB Notion Extension

This notebook demonstrates how to build a complete ETL pipeline that extracts data from various sources and loads it into Notion.

## Setup

```{python}
#| eval: false
import duckdb
import pandas as pd
from pathlib import Path
import os

con = duckdb.connect()
# con.execute("LOAD notion")

print("âœ“ DuckDB initialized")
```

## Extract: Reading from Multiple Sources

### Source 1: CSV Files

```{python}
#| eval: false
# Create sample CSV data
sample_data = pd.DataFrame({
    'product_name': ['Widget A', 'Widget B', 'Widget C'],
    'category': ['Electronics', 'Home', 'Electronics'],
    'price': [29.99, 19.99, 49.99],
    'stock': [100, 50, 75],
    'active': [True, True, False]
})

sample_data.to_csv('products.csv', index=False)
print("âœ“ Sample CSV created")

# Read CSV with DuckDB
products = con.execute("""
    SELECT * FROM read_csv('products.csv')
""").df()

products
```

### Source 2: Parquet Files

```{python}
#| eval: false
# Create sample Parquet data
orders = pd.DataFrame({
    'order_id': [1, 2, 3, 4, 5],
    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],
    'quantity': [2, 1, 3, 1, 2],
    'order_date': pd.date_range('2024-01-01', periods=5)
})

orders.to_parquet('orders.parquet')
print("âœ“ Sample Parquet created")

# Read Parquet with DuckDB
orders_df = con.execute("""
    SELECT * FROM read_parquet('orders.parquet')
""").df()

orders_df
```

### Source 3: JSON Data

```{python}
#| eval: false
import json

# Create sample JSON data
customers = [
    {'id': 1, 'name': 'Acme Corp', 'segment': 'Enterprise'},
    {'id': 2, 'name': 'Small Biz', 'segment': 'SMB'},
    {'id': 3, 'name': 'Mega Corp', 'segment': 'Enterprise'}
]

with open('customers.json', 'w') as f:
    json.dump(customers, f)

print("âœ“ Sample JSON created")

# Read JSON with DuckDB
customers_df = con.execute("""
    SELECT * FROM read_json('customers.json')
""").df()

customers_df
```

## Transform: Data Cleaning and Aggregation

```{python}
#| eval: false
# Create a comprehensive dataset
con.execute("""
    CREATE TABLE sales_summary AS
    SELECT
        p.product_name,
        p.category,
        p.price,
        COUNT(o.order_id) as total_orders,
        SUM(o.quantity) as total_quantity_sold,
        ROUND(p.price * SUM(o.quantity), 2) as total_revenue,
        p.stock as current_stock,
        CASE
            WHEN p.stock < 60 THEN 'Low Stock'
            WHEN p.stock < 90 THEN 'Medium Stock'
            ELSE 'High Stock'
        END as stock_status
    FROM read_csv('products.csv') p
    LEFT JOIN read_parquet('orders.parquet') o
        ON p.product_name = o.product
    GROUP BY p.product_name, p.category, p.price, p.stock
""")

sales_summary = con.execute("SELECT * FROM sales_summary").df()
print("=== Sales Summary ===")
sales_summary
```

## Load: Writing to Notion

### Prepare Data for Notion

```{python}
#| eval: false
# Create a table optimized for Notion
con.execute("""
    CREATE TABLE notion_products AS
    SELECT
        product_name as "Product Name",
        CAST(total_revenue AS INTEGER) as "Revenue",
        total_orders as "Order Count",
        stock_status = 'Low Stock' as "Needs Restock"
    FROM sales_summary
""")

notion_ready = con.execute("SELECT * FROM notion_products").df()
print("=== Data Ready for Notion ===")
notion_ready
```

### Write to Notion Database

```{python}
#| eval: false
# Write to Notion
database_id = os.getenv('NOTION_DATABASE_ID', 'your_database_id')

# Uncomment when extension is loaded
# con.execute(f"""
#     COPY notion_products
#     TO '{database_id}'
#     (FORMAT notion)
# """)

print(f"âœ“ Would write {len(notion_ready)} rows to Notion database")
print(f"  Database ID: {database_id}")
```

## Incremental Updates

```{python}
#| eval: false
# Track what's already in Notion
existing_data = con.execute(f"""
    SELECT * FROM read_notion('{database_id}')
""").df()

# Find new records
new_records = con.execute("""
    SELECT n.*
    FROM notion_products n
    LEFT JOIN existing_data e
        ON n."Product Name" = e.title
    WHERE e.id IS NULL
""").df()

print(f"Found {len(new_records)} new records to add")
new_records
```

## Automated Pipeline

```{python}
#| eval: false
def etl_pipeline(source_files, notion_db_id):
    """
    Complete ETL pipeline from files to Notion
    """
    print("ðŸ”„ Starting ETL Pipeline...")

    # Extract
    print("  ðŸ“¥ Extracting data...")
    data = con.execute(f"""
        SELECT * FROM read_csv('{source_files['csv']}')
    """).df()

    # Transform
    print("  ðŸ”§ Transforming data...")
    transformed = data.copy()
    # Add your transformation logic here

    # Load
    print("  ðŸ“¤ Loading to Notion...")
    con.register('transformed_data', transformed)
    # con.execute(f"COPY transformed_data TO '{notion_db_id}' (FORMAT notion)")

    print("âœ… ETL Pipeline complete!")
    return len(transformed)

# Run the pipeline
source_files = {
    'csv': 'products.csv'
}
rows_processed = etl_pipeline(source_files, database_id)
print(f"Processed {rows_processed} rows")
```

## Scheduling with Cron

Save this as a Python script and schedule it:

```{python}
#| eval: false
#| code-fold: true
script_content = """
#!/usr/bin/env python3
import duckdb
import os
from datetime import datetime

def run_etl():
    con = duckdb.connect()
    con.execute("LOAD notion")

    # Your ETL logic here
    result = con.execute('''
        COPY (SELECT * FROM read_csv('data.csv'))
        TO 'notion_db_id'
        (FORMAT notion)
    ''')

    print(f"[{datetime.now()}] ETL completed: {result}")

if __name__ == "__main__":
    run_etl()
"""

with open('etl_script.py', 'w') as f:
    f.write(script_content)

print("âœ“ ETL script created: etl_script.py")
print("\nTo schedule with cron:")
print("  crontab -e")
print("  # Add: 0 */6 * * * /path/to/etl_script.py")
```

## Cleanup

```{python}
#| eval: false
# Clean up sample files
import os
for file in ['products.csv', 'orders.parquet', 'customers.json']:
    if os.path.exists(file):
        os.remove(file)

print("âœ“ Cleanup complete")
```

## Summary

This notebook demonstrated:

- âœ… Extracting data from CSV, Parquet, and JSON
- âœ… Transforming and aggregating data with SQL
- âœ… Loading data into Notion databases
- âœ… Handling incremental updates
- âœ… Building automated ETL pipelines

Next steps: Deploy this pipeline to production with proper error handling and monitoring!
